{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging/joining tables\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3519 entries, 0 to 3518\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   rid      3519 non-null   object\n",
      " 1   vid      3519 non-null   object\n",
      " 2   owner    3519 non-null   object\n",
      " 3   address  3519 non-null   object\n",
      " 4   zip      3519 non-null   object\n",
      "dtypes: object(5)\n",
      "memory usage: 137.6+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3519 entries, 0 to 3518\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   vid        3519 non-null   object\n",
      " 1   make       3519 non-null   object\n",
      " 2   model      3519 non-null   object\n",
      " 3   year       3519 non-null   int64 \n",
      " 4   fuel_type  3519 non-null   object\n",
      " 5   owner      3519 non-null   object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 165.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import tables\n",
    "\n",
    "taxi_owners = pd.read_pickle(\"D:/Data Science/Data Sources/Data Camp/taxi_owners.p\")\n",
    "taxi_veh = pd.read_pickle(\"D:/Data Science/Data Sources/Data Camp/taxi_vehicles.p\")\n",
    "\n",
    "print(taxi_owners.info())\n",
    "print(taxi_veh.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        rid   vid         owner_own                 address    zip    make  \\\n",
      "0     T6285  6285    AGEAN TAXI LLC     4536 N. ELSTON AVE.  60630  NISSAN   \n",
      "1     T4862  4862      MANGIB CORP.  5717 N. WASHTENAW AVE.  60659   HONDA   \n",
      "2     T1495  1495     FUNRIDE, INC.     3351 W. ADDISON ST.  60618  TOYOTA   \n",
      "3     T4231  4231      ALQUSH CORP.   6611 N. CAMPBELL AVE.  60645  TOYOTA   \n",
      "4     T5971  5971    EUNIFFORD INC.     3351 W. ADDISON ST.  60618  TOYOTA   \n",
      "...     ...   ...               ...                     ...    ...     ...   \n",
      "3514  T4453  4453   IMAGIN CAB CORP     3351 W. ADDISON ST.  60618    FORD   \n",
      "3515   T121   121  TRIBECA CAB CORP     4536 N. ELSTON AVE.  60630    FORD   \n",
      "3516  T3465  3465  AMIR EXPRESS INC     3351 W. ADDISON ST.  60618  TOYOTA   \n",
      "3517  T1962  1962  KARY CAB COMPANY     4707 N. KENTON AVE.  60630  TOYOTA   \n",
      "3518  T1031  1031       NECT 42 LLC    6500 N. WESTERN AVE.  60645    FORD   \n",
      "\n",
      "       model  year fuel_type         owner_veh  \n",
      "0     ALTIMA  2011    HYBRID    AGEAN TAXI LLC  \n",
      "1        CRV  2014  GASOLINE      MANGIB CORP.  \n",
      "2     SIENNA  2015  GASOLINE     FUNRIDE, INC.  \n",
      "3      CAMRY  2014    HYBRID      ALQUSH CORP.  \n",
      "4     SIENNA  2015  GASOLINE    EUNIFFORD INC.  \n",
      "...      ...   ...       ...               ...  \n",
      "3514  ESCAPE  2010    HYBRID   IMAGIN CAB CORP  \n",
      "3515   C-MAX  2014    HYBRID  TRIBECA CAB CORP  \n",
      "3516   CAMRY  2014    HYBRID  AMIR EXPRESS INC  \n",
      "3517   CAMRY  2014    HYBRID  KARY CAB COMPANY  \n",
      "3518  FUSION  2014    HYBRID       NECT 42 LLC  \n",
      "\n",
      "[3519 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# merge tables inner join\n",
    "\n",
    "taxi_own_veh = taxi_owners.merge(taxi_veh, on='vid', suffixes=('_own','_veh')) # merge returns rows where values match in both tables\n",
    "\n",
    "print(taxi_own_veh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HYBRID                    2792\n",
      "GASOLINE                   611\n",
      "FLEX FUEL                   89\n",
      "COMPRESSED NATURAL GAS      27\n",
      "Name: fuel_type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(taxi_own_veh['fuel_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   ward      50 non-null     object\n",
      " 1   alderman  50 non-null     object\n",
      " 2   address   50 non-null     object\n",
      " 3   zip       50 non-null     object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50 entries, 0 to 49\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   ward      50 non-null     object\n",
      " 1   pop_2000  50 non-null     int64 \n",
      " 2   pop_2010  50 non-null     int64 \n",
      " 3   change    50 non-null     object\n",
      " 4   address   50 non-null     object\n",
      " 5   zip       50 non-null     object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 2.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import new tables\n",
    "\n",
    "ward = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\ward.p\")\n",
    "\n",
    "census = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\census.p\")\n",
    "\n",
    "print(ward.info())\n",
    "print(census.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wards_census table shape: (50, 9)\n"
     ]
    }
   ],
   "source": [
    "# inner join\n",
    "\n",
    "wards_census = ward.merge(census, on = 'ward')\n",
    "\n",
    "print('wards_census table shape:', wards_census.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'licenses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Data Science\\Data Science Projects\\DS\\Data camp\\python\\joining_df.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data%20Science/Data%20Science%20Projects/DS/Data%20camp/python/joining_df.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Merge the licenses and biz_owners table on account\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Data%20Science/Data%20Science%20Projects/DS/Data%20camp/python/joining_df.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m licenses_owners \u001b[39m=\u001b[39m licenses\u001b[39m.\u001b[39mmerge(biz_owners, on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39maccount\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data%20Science/Data%20Science%20Projects/DS/Data%20camp/python/joining_df.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Group the results by title then count the number of accounts\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Data%20Science/Data%20Science%20Projects/DS/Data%20camp/python/joining_df.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m counted_df \u001b[39m=\u001b[39m licenses_owners\u001b[39m.\u001b[39mgroupby(\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39magg({\u001b[39m'\u001b[39m\u001b[39maccount\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mcount\u001b[39m\u001b[39m'\u001b[39m})\n",
      "\u001b[1;31mNameError\u001b[0m: name 'licenses' is not defined"
     ]
    }
   ],
   "source": [
    "# Merge the licenses and biz_owners table on account\n",
    "licenses_owners = licenses.merge(biz_owners, on='account')\n",
    "\n",
    "# Group the results by title then count the number of accounts\n",
    "counted_df = licenses_owners.groupby('title').agg({'account':'count'})\n",
    "\n",
    "# Sort the counted_df in desending order\n",
    "sorted_df = counted_df.sort_values(by ='account',ascending=False)\n",
    "\n",
    "# Use .head() method to print the first few rows of sorted_df\n",
    "print(sorted_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             income\n",
      "alderman                           \n",
      "Ameya Pawar                 66246.0\n",
      "Anthony A. Beale            38206.0\n",
      "Anthony V. Napolitano       82226.0\n",
      "Ariel E. Reyboras           41307.0\n",
      "Brendan Reilly             110215.0\n",
      "Brian Hopkins               87143.0\n",
      "Carlos Ramirez-Rosa         66246.0\n",
      "Carrie M. Austin            38206.0\n",
      "Chris Taliaferro            55566.0\n",
      "Daniel \"Danny\" Solis        41226.0\n",
      "David H. Moore              33304.0\n",
      "Deborah Mell                66246.0\n",
      "Debra L. Silverstein        50554.0\n",
      "Derrick G. Curtis           65770.0\n",
      "Edward M. Burke             42335.0\n",
      "Emma M. Mitts               36283.0\n",
      "George Cardenas             33959.0\n",
      "Gilbert Villegas            41307.0\n",
      "Gregory I. Mitchell         24941.0\n",
      "Harry Osterman              45442.0\n",
      "Howard B. Brookins, Jr.     33304.0\n",
      "James Cappleman             79565.0\n",
      "Jason C. Ervin              41226.0\n",
      "Joe Moore                   39163.0\n",
      "John S. Arena               70122.0\n",
      "Leslie A. Hairston          28024.0\n",
      "Margaret Laurino            70122.0\n",
      "Marty Quinn                 67045.0\n",
      "Matthew J. O'Shea           59488.0\n",
      "Michael R. Zalewski         42335.0\n",
      "Michael Scott, Jr.          31445.0\n",
      "Michelle A. Harris          32558.0\n",
      "Michelle Smith             100116.0\n",
      "Milagros \"Milly\" Santiago   41307.0\n",
      "Nicholas Sposato            62223.0\n",
      "Pat Dowell                  46340.0\n",
      "Patrick Daley Thompson      41226.0\n",
      "Patrick J. O'Connor         50554.0\n",
      "Proco \"Joe\" Moreno          87143.0\n",
      "Raymond A. Lopez            33959.0\n",
      "Ricardo Munoz               31445.0\n",
      "Roberto Maldonado           68223.0\n",
      "Roderick T. Sawyer          32558.0\n",
      "Scott Waguespack            68223.0\n",
      "Susan Sadlowski Garza       38417.0\n",
      "Tom Tunney                  88708.0\n",
      "Toni L. Foulkes             27573.0\n",
      "Walter Burnett, Jr.         87143.0\n",
      "William D. Burns           107811.0\n",
      "Willie B. Cochran           28024.0\n"
     ]
    }
   ],
   "source": [
    "#merging multiple tables with '\\'\n",
    "\n",
    "wards = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\ward.p\")\n",
    "zip_demo = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\zip_demo.p\")\n",
    "\n",
    "# Merge licenses and zip_demo, on zip; and merge the wards on ward\n",
    "licenses_zip_ward = licenses.merge(zip_demo, on = 'zip') \\\n",
    "            \t\t\t.merge(wards, on = 'ward')\n",
    " \n",
    "# Print the results by alderman and show median income\n",
    "print(licenses_zip_ward.groupby('alderman').agg({'income':'median'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ward  pop_2010  vacant  account\n",
      "47    7     51581      19       80\n",
      "12   20     52372      15      123\n",
      "1    10     51535      14      130\n",
      "16   24     54909      13       98\n",
      "7    16     51954      13      156\n"
     ]
    }
   ],
   "source": [
    "# merging multiple tables inner join\n",
    "\n",
    "land_use = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\land_use.p\")\n",
    "\n",
    "# Merge land_use and census and merge result with licenses including suffixes\n",
    "land_cen_lic = land_use.merge(census, on='ward') \\\n",
    "                    .merge(licenses, on='ward', suffixes=('_cen','_lic'))\n",
    "\n",
    "# Group by ward, pop_2010, and vacant, then count the # of accounts\n",
    "pop_vac_lic = land_cen_lic.groupby(['ward','pop_2010','vacant'], \n",
    "                                   as_index=False).agg({'account':'count'})\n",
    "\n",
    "# Sort pop_vac_lic and print the results\n",
    "sorted_pop_vac_lic = pop_vac_lic.sort_values(by = ['vacant','account','pop_2010'], \n",
    "                                             ascending=(False,True,True))\n",
    "\n",
    "# Print the top few rows of sorted_pop_vac_lic\n",
    "print(sorted_pop_vac_lic.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id     budget       revenue\n",
      "0   19995  237000000  2.787965e+09\n",
      "1     285  300000000  9.610000e+08\n",
      "2  206647  245000000  8.806746e+08\n",
      "3   49026  250000000  1.084939e+09\n",
      "4   49529  260000000  2.841391e+08\n",
      "      id                 title  popularity release_date\n",
      "0    257          Oliver Twist   20.415572   2005-09-23\n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12\n",
      "2  38365             Grown Ups   38.864027   2010-06-24\n",
      "3   9672              Infamous    3.680896   2006-11-16\n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17\n"
     ]
    }
   ],
   "source": [
    "# left join\n",
    "\n",
    "financials = pd.read_pickle(\"D:/Data Science/Data Sources/Data Camp/financials.p\")\n",
    "movies = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\movies.p\")\n",
    "\n",
    "print(financials.head())\n",
    "print(movies.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1574\n"
     ]
    }
   ],
   "source": [
    "# Merge the movies table with the financials table with a left/right join\n",
    "movies_financials = movies.merge(financials, on='id', how='left')\n",
    "\n",
    "# Count the number of rows in the budget column that are missing\n",
    "number_of_missing_fin = movies_financials['budget'].isna().sum()\n",
    "\n",
    "# Print the number of movies missing financials\n",
    "print(number_of_missing_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id department_dir job_dir           name_dir department_crew  \\\n",
      "0  19995        Editing  Editor  Stephen E. Rivkin         Editing   \n",
      "1  19995        Editing  Editor  Stephen E. Rivkin           Sound   \n",
      "2  19995        Editing  Editor  Stephen E. Rivkin      Production   \n",
      "3  19995        Editing  Editor  Stephen E. Rivkin       Directing   \n",
      "4  19995        Editing  Editor  Stephen E. Rivkin         Writing   \n",
      "\n",
      "         job_crew          name_crew  \n",
      "0          Editor  Stephen E. Rivkin  \n",
      "1  Sound Designer  Christopher Boyes  \n",
      "2         Casting          Mali Finn  \n",
      "3        Director      James Cameron  \n",
      "4          Writer      James Cameron  \n"
     ]
    }
   ],
   "source": [
    "# self join\n",
    "\n",
    "crews = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\crews.p\")\n",
    "\n",
    "# Merge the crews table to itself\n",
    "crews_self_merged = crews.merge(crews, left_on = 'id', right_on = 'id', suffixes = ('_dir', '_crew'))\n",
    "\n",
    "print(crews_self_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id department_dir   job_dir       name_dir department_crew  \\\n",
      "156  19995      Directing  Director  James Cameron         Editing   \n",
      "157  19995      Directing  Director  James Cameron           Sound   \n",
      "158  19995      Directing  Director  James Cameron      Production   \n",
      "160  19995      Directing  Director  James Cameron         Writing   \n",
      "161  19995      Directing  Director  James Cameron             Art   \n",
      "\n",
      "           job_crew          name_crew  \n",
      "156          Editor  Stephen E. Rivkin  \n",
      "157  Sound Designer  Christopher Boyes  \n",
      "158         Casting          Mali Finn  \n",
      "160          Writer      James Cameron  \n",
      "161    Set Designer    Richard F. Mays  \n"
     ]
    }
   ],
   "source": [
    "# Create a Boolean index to select the appropriate\n",
    "boolean_filter = ((crews_self_merged['job_dir'] == 'Director') & \n",
    "     (crews_self_merged['job_crew'] != 'Director'))\n",
    "direct_crews = crews_self_merged[boolean_filter]\n",
    "\n",
    "print(direct_crews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                 title  popularity release_date  vote_average  \\\n",
      "0    257          Oliver Twist   20.415572   2005-09-23           6.7   \n",
      "1  14290  Better Luck Tomorrow    3.877036   2002-01-12           6.5   \n",
      "2  38365             Grown Ups   38.864027   2010-06-24           6.0   \n",
      "3   9672              Infamous    3.680896   2006-11-16           6.4   \n",
      "4  12819       Alpha and Omega   12.300789   2010-09-17           5.3   \n",
      "\n",
      "   vote_count  \n",
      "0       274.0  \n",
      "1        27.0  \n",
      "2      1705.0  \n",
      "3        60.0  \n",
      "4       124.0  \n"
     ]
    }
   ],
   "source": [
    "# merging on indexes of DF\n",
    "\n",
    "ratings = pd.read_pickle(\"D:/Data Science/Data Sources/Data Camp/ratings.p\")\n",
    "\n",
    "# Merge to the movies table the ratings table on the index\n",
    "movies_ratings = movies.merge(ratings, left_on = 'id', right_on = 'id')\n",
    "\n",
    "# Print the first few rows of movies_ratings\n",
    "print(movies_ratings.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id         title  sequel       budget       revenue\n",
      "0  19995        Avatar    <NA>  237000000.0  2.787965e+09\n",
      "1    862     Toy Story     863   30000000.0  3.735540e+08\n",
      "2    863   Toy Story 2   10193   90000000.0  4.973669e+08\n",
      "3    597       Titanic    <NA>  200000000.0  1.845034e+09\n",
      "4  24428  The Avengers    <NA>  220000000.0  1.519558e+09\n",
      "            title_org        title_seq          diff\n",
      "28  Jurassic Park III   Jurassic World  1.144748e+09\n",
      "26      Batman Begins  The Dark Knight  6.303398e+08\n",
      "11         Iron Man 2       Iron Man 3  5.915067e+08\n",
      "1         Toy Story 2      Toy Story 3  5.696028e+08\n",
      "14  Quantum of Solace          Skyfall  5.224703e+08\n"
     ]
    }
   ],
   "source": [
    "sequels = pd.read_pickle(\"D:\\Data Science\\Data Sources\\Data Camp\\sequels.p\")\n",
    "\n",
    "# Merge sequels and financials on index id\n",
    "sequels_fin = sequels.merge(financials, on = 'id', how = 'left')\n",
    "\n",
    "print(sequels_fin.head())\n",
    "\n",
    "# Self merge with suffixes as inner join with left on sequel and right on id\n",
    "orig_seq = sequels_fin.merge(sequels_fin, left_on='sequel', \n",
    "                             right_on='id',\n",
    "                             suffixes=('_org','_seq'))\n",
    "\n",
    "\n",
    "# Add calculation to subtract revenue_org from revenue_seq \n",
    "orig_seq['diff'] = orig_seq['revenue_seq'] - orig_seq['revenue_org']\n",
    "\n",
    "# Select the title_org, title_seq, and diff \n",
    "titles_diff = orig_seq[['title_org','title_seq','diff']]\n",
    "\n",
    "# Print the first rows of the sorted titles_diff\n",
    "print(titles_diff.sort_values(by = 'diff', ascending = False).head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating vertically\n",
    "\n",
    "# pd.concat([df1,df2,df3]) - must be the same columns and number of columns\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging in order using pd.merge_ordered\n",
    "\n",
    "# great for time series data\n",
    "\n",
    "# pd.merge_ordered(df,df,left_on =, right_on =, how=)\n",
    "\n",
    "# pd.merge_asof(df,df, on, left_on = , right_on=,)\n",
    "\n",
    "#df.query() -  much like a sql statement df.query('col1 > xyz and/or (col2 == xyz or col 3 > xyz')\n",
    "\n",
    "# df.melt() - reshapes df - transforms wide to long df and vice versa\n",
    "    # .melt(id_vars = [], value_vars = [], var_name =''), value_name = '')\n",
    "        # where id_vars = identifier variables - cols we do not want to change; value_vars = control cols to unpivot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
